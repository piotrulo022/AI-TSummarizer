{"cells":[{"cell_type":"markdown","metadata":{},"source":["# *Training selfmade transformer*\n","\n","## Piotr Szyszka\n","### ***Lublin University of Technology***\n","### *Engineering and Data Analysis, 2024*\n","### *Advanced Machine Learning Methods*\n"]},{"cell_type":"markdown","metadata":{},"source":["This notebook is used to train the model."]},{"cell_type":"markdown","metadata":{},"source":["# Necessary imports"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/piotr/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import math\n","\n","from torch.utils.tensorboard import SummaryWriter\n","import torchmetrics\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset, random_split\n","\n","from datasets import Dataset as DS\n","\n","\n","\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace, WhitespaceSplit\n","\n","from tqdm import tqdm\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UGoD6MAhdzL4"},"source":["# Model definition"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:01.904043Z","iopub.status.busy":"2024-06-10T13:53:01.903745Z","iopub.status.idle":"2024-06-10T13:53:03.859287Z","shell.execute_reply":"2024-06-10T13:53:03.858429Z","shell.execute_reply.started":"2024-06-10T13:53:01.904016Z"},"id":"vp71T5LZdzL6","trusted":true},"outputs":[],"source":["\n","###############################################################################\n","# Utility layers\n","\n","class LayerNormalization(nn.Module):\n","\n","    def __init__(self, features: int, eps:float=10**-6) -> None:\n","        super().__init__()\n","        self.eps = eps\n","        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n","        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n","\n","    def forward(self, x):\n","        # x: (batch, seq_len, hidden_size)\n","         # Keep the dimension for broadcasting\n","        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n","        # Keep the dimension for broadcasting\n","        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n","        # eps is to prevent dividing by zero or when std is very small\n","        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n","\n","\n","class ResidualConnection(nn.Module):\n","        def __init__(self, features: int, dropout: float) -> None:\n","            super().__init__()\n","            self.dropout = nn.Dropout(dropout)\n","            self.norm = LayerNormalization(features)\n","\n","        def forward(self, x, sublayer):\n","            return x + self.dropout(sublayer(self.norm(x)))\n","\n","# Input Embedding Layer\n","class InputEmbeddings(nn.Module):\n","    \"\"\"\n","    Input embedding layer - very first layer that transforms tokens into its numerical representation.\n","\n","    # Parameters:\n","\n","    - `d_model (int)` - dimension of the model (output size of embedidng layer)\n","\n","    - `vocab_size (int)` - number of unique words in vocabulary.\n","\n","\n","    \"\"\"\n","    def __init__(self, d_model: int, vocab_size: int) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.vocab_size = vocab_size\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        return self.embedding(x) * math.sqrt(self.d_model) # multiply by sqrt(d_model) as in original paper\n","\n","# 0.2) Postional encoding\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Positional Encoding layer - stores information of order of words occuring in the sequence.\n","\n","    # Parameters\n","\n","    - `d_model (int)` - dimension of the model (output size of embedidng layer),\n","\n","    - `seq_len (int)` - fixed number of tokens for every sequence of words.\n","\n","    - `dropout (float)` - dropout factor added to encoding layer; applied to avoid overfitting.\n","\n","    \"\"\"\n","    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.seq_len = seq_len\n","        self.dropout = nn.Dropout(dropout)\n","\n","        pe = torch.zeros(seq_len, d_model) # positional encoding matrix\n","\n","        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (0, 1, ..., seq_len - 1)\n","\n","\n","        # following formulas given in original paper\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n","\n","        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n","\n","        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n","\n","\n","\n","        pe = pe.unsqueeze(0) # add batch dimension - (1, seq_len, d_model)\n","\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n","\n","\n","        return self.dropout(x)\n","\n","\n","# Multi-Head Attention\n","class MultiHeadAttentionBlock(nn.Module):\n","    \"\"\"\n","    Multi-Head Attention block - implementation of multi-head attention mechanism.\n","\n","    # Parameters:\n","\n","    - `d_model (int)` - dimension of the model (output size of embedidng layer),\n","\n","    - `h (int)` - number of attention heads,\n","\n","    - `dropout (float)` - dropout factor.\n","\n","    \"\"\"\n","    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n","        super().__init__()\n","\n","        assert d_model % h == 0, \"Model dimension (d_model) must be divisible by number of attention heads (h)\"\n","\n","        self.d_model = d_model\n","        self.h = h\n","\n","        self.head_dim = d_model // h # dim of vector seen by each head\n","\n","        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq - Query\n","        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk - Key\n","        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv - Value\n","        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    @staticmethod\n","    def attention(query, key, value, mask, dropout: nn.Dropout):\n","        d_k = query.shape[-1]\n","\n","        # following formula in paper\n","        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n","        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n","        if mask is not None:\n","            attention_scores.masked_fill_(mask == 0, -1e9) # it'll become 0 after softmax\n","\n","        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n","        if dropout is not None:\n","            attention_scores = dropout(attention_scores)\n","        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n","        return (attention_scores @ value), attention_scores\n","\n","    def forward(self, q, k, v, mask):\n","        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","\n","        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n","\n","\n","        # split (query, key, value) equally among attention heads:\n","\n","        query = query.view(query.shape[0], query.shape[1], self.h, self.head_dim).transpose(1, 2)\n","        key = key.view(key.shape[0], key.shape[1], self.h, self.head_dim).transpose(1, 2)\n","        value = value.view(value.shape[0], value.shape[1], self.h, self.head_dim).transpose(1, 2)\n","\n","        # calculate attention\n","        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n","\n","        # combine all the heads together\n","        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n","        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.head_dim)\n","\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        return self.w_o(x)\n","\n","\n","# Feed Forward Block\n","class FeedForwardBlock(nn.Module):\n","    \"\"\"\n","    Feed Forward Block (Linear -> ReLu -> Dropout -> Linear)\n","\n","    # Parameters:\n","\n","    - `d_model (int)` - model dimension,\n","\n","    - `d_ff (int)` - feed forward dimension,\n","\n","    - `dropout (float)` - dropout factor.\n","\n","    \"\"\"\n","    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n","        super().__init__()\n","        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n","\n","    def forward(self, x):\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n","        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n","\n","\n","# Encoder block and encoder\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n","\n","    def forward(self, x, src_mask):\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n","        x = self.residual_connections[1](x, self.feed_forward_block)\n","        return x\n","\n","class Encoder(nn.Module):\n","    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization(features)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","# Decoder block and decoder\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.cross_attention_block = cross_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n","        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n","        x = self.residual_connections[2](x, self.feed_forward_block)\n","        return x\n","\n","class Decoder(nn.Module):\n","    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization(features)\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, encoder_output, src_mask, tgt_mask)\n","        return self.norm(x)\n","\n","\n","\n","# Projection layer\n","class ProjectionLayer(nn.Module):\n","    def __init__(self, d_model, vocab_size) -> None:\n","        super().__init__()\n","        self.proj = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x) -> None:\n","        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n","        return self.proj(x)\n","\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.tgt_embed = tgt_embed\n","        self.src_pos = src_pos\n","        self.tgt_pos = tgt_pos\n","        self.projection_layer = projection_layer\n","\n","    def encode(self, src, src_mask):\n","        # (batch, seq_len, d_model)\n","        src = self.src_embed(src)\n","        src = self.src_pos(src)\n","        return self.encoder(src, src_mask)\n","\n","    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n","        # (batch, seq_len, d_model)\n","        tgt = self.tgt_embed(tgt)\n","        tgt = self.tgt_pos(tgt)\n","        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n","\n","    def project(self, x):\n","        # (batch, seq_len, vocab_size)\n","        return self.projection_layer(x)\n","\n","\n","\n","def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n","    # Create the embedding layers\n","    src_embed = InputEmbeddings(d_model, src_vocab_size)\n","    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n","\n","    # Create the positional encoding layers\n","    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n","    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n","\n","    # Create the encoder blocks\n","    encoder_blocks = []\n","    for _ in range(N):\n","        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n","        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n","        encoder_blocks.append(encoder_block)\n","\n","    # Create the decoder blocks\n","    decoder_blocks = []\n","    for _ in range(N):\n","        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n","        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n","        decoder_blocks.append(decoder_block)\n","\n","    # Create the encoder and decoder\n","    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n","    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n","\n","    # Create the projection layer\n","    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n","\n","    # Create the transformer\n","    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n","\n","    # Initialize the parameters\n","    for p in transformer.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","\n","    return transformer"]},{"cell_type":"markdown","metadata":{"id":"WhASAemWdzL9"},"source":["# Dataset and tokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:03.878858Z","iopub.status.busy":"2024-06-10T13:53:03.878514Z","iopub.status.idle":"2024-06-10T13:53:03.883069Z","shell.execute_reply":"2024-06-10T13:53:03.882219Z","shell.execute_reply.started":"2024-06-10T13:53:03.878831Z"},"id":"4uydXkGLdzL-","trusted":true},"outputs":[],"source":["def yield_sentences(ds):\n","    for item in ds:\n","        yield item['Context']\n","        yield item['Response']"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:03.884380Z","iopub.status.busy":"2024-06-10T13:53:03.884103Z","iopub.status.idle":"2024-06-10T13:53:03.892053Z","shell.execute_reply":"2024-06-10T13:53:03.891285Z","shell.execute_reply.started":"2024-06-10T13:53:03.884355Z"},"id":"8ZcXu9s7dzL-","trusted":true},"outputs":[],"source":["def create_tokenizer(ds: DS) -> Tokenizer:\n","\n","    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","    tokenizer.pre_tokenizer = WhitespaceSplit() # split words based on whitespaces between\n","\n","    trainer = WordLevelTrainer(special_tokens = ['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency = 2, vocab_size = 50_000)\n","\n","    tokenizer.train_from_iterator(yield_sentences(ds), trainer = trainer)\n","\n","\n","    return tokenizer"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:03.893728Z","iopub.status.busy":"2024-06-10T13:53:03.893326Z","iopub.status.idle":"2024-06-10T13:53:03.921986Z","shell.execute_reply":"2024-06-10T13:53:03.921012Z","shell.execute_reply.started":"2024-06-10T13:53:03.893698Z"},"id":"sNDKnS4wdzL_","trusted":true},"outputs":[],"source":["class MHDataset(Dataset):\n","    def __init__(self, ds: DS, tokenizer, src_seq_len, tgt_seq_len) -> None:\n","        super().__init__()\n","        self.ds = ds\n","\n","        self.src_seq_len = src_seq_len\n","        self.tgt_seq_len = tgt_seq_len\n","\n","        self.tokenizer = tokenizer\n","\n","        self.sos_token = torch.tensor([tokenizer.token_to_id('[SOS]')], dtype = torch.int64)\n","        self.eos_token = torch.tensor([tokenizer.token_to_id('[EOS]')], dtype = torch.int64)\n","        self.pad_token = torch.tensor([tokenizer.token_to_id('[PAD]')], dtype = torch.int64)\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, index):\n","        src_target_pair = self.ds[index]\n","        src_text, tgt_text = src_target_pair['Context'], src_target_pair['Response']\n","\n","        enc_input_tokens = self.tokenizer.encode(src_text).ids\n","        dec_input_tokens = self.tokenizer.encode(tgt_text).ids\n","\n","\n","        # padding\n","        enc_num_padding_tokens = self.src_seq_len - len(enc_input_tokens) - 2 # -2 : [SOS], [EOS]\n","        dec_num_padding_tokens = self.tgt_seq_len - len(dec_input_tokens) - 1 # we do not want to have [EOS] ([SOS] will be always starting token, what is necessary for decoding)\n","        if enc_num_padding_tokens < 0:\n","            enc_input_tokens = enc_input_tokens[:self.src_seq_len - 2]  # -2 for [SOS] and [EOS]\n","            encoder_input = torch.cat(\n","                [\n","                    self.sos_token,\n","                    torch.tensor(enc_input_tokens, dtype=torch.int64),\n","                    self.eos_token\n","                ], dim = 0\n","            )\n","        else:\n","            encoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(enc_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64)\n","            ], dim = 0\n","        )\n","\n","\n","        if dec_num_padding_tokens <= 0:\n","            dec_input_tokens = dec_input_tokens[:self.tgt_seq_len - 1]  # -1 for [SOS]\n","            decoder_input = torch.cat(\n","                [\n","                    self.sos_token,\n","                    torch.tensor(dec_input_tokens, dtype = torch.int64),\n","                ], dim = 0\n","            )\n","\n","            label = torch.cat(\n","              [\n","                  torch.tensor(dec_input_tokens, dtype = torch.int64),\n","                  self.eos_token\n","              ], dim = 0\n","            )\n","        else:\n","            decoder_input = torch.cat(\n","                [\n","                    self.sos_token,\n","                    torch.tensor(dec_input_tokens, dtype = torch.int64),\n","                    torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n","                ], dim = 0\n","            )\n","            label = torch.cat(\n","                [\n","                    torch.tensor(dec_input_tokens, dtype = torch.int64),\n","                    self.eos_token,\n","                    torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64)\n","                ], dim = 0\n","            )\n","\n","\n","        assert encoder_input.size(0) == self.src_seq_len\n","        assert decoder_input.size(0) == self.tgt_seq_len\n","        assert label.size(0) == self.tgt_seq_len\n","\n","        return {'encoder_input': encoder_input,\n","                'decoder_input': decoder_input,\n","                'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n","                'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & decoder_mask(decoder_input.size(0)),\n","                'label': label,\n","                'src_text': src_text,\n","                'tgt_text': tgt_text\n","                }\n","\n","\n","def decoder_mask(size: int) -> torch.Tensor:\n","    \"\"\"\n","    Generates a mask for the decoder to prevent attention to subsequent positions in the sequence.\n","\n","    This mask is a lower triangular matrix of zeros and ones, ensuring that the decoder at a given position can only\n","    attend to previous positions in the sequence.\n","\n","    Args:\n","        size (int): The size of the mask, typically the length of the target sequence.\n","\n","    Returns:\n","        torch.Tensor: A mask of shape (1, size, size), where True values indicate positions that can be attended to.\n","    \"\"\"\n","    return torch.tril(torch.ones(1, size, size), diagonal=1).type(torch.int)\n","\n","\n","\n","def prepare_dataset(train_size: float, src_seq_len: int = None, tgt_seq_len: int = None, train_batch_size: int = 16, val_batch_size: int = 16):\n","\n","    assert train_size > 0 and train_size < 1, \"Train size can't be outside (0; 1) range \"\n","\n","    print('Fetching dataset')\n","\n","    dataset = DS.from_parquet('./MH_train.parquet')\n","    \n","    print('Dataset loaded successfully!')\n","\n","    tokenizer = create_tokenizer(dataset)\n","\n","    print('Creating train-vali split')\n","    train_ds_size = int(train_size * len(dataset))\n","    val_ds_size = len(dataset) - train_ds_size\n","\n","    print('Training samples:', train_ds_size)\n","    print('Validation samples', val_ds_size)\n","\n","\n","    train_ds, val_ds = random_split(dataset, [train_ds_size, val_ds_size])\n","\n","    if src_seq_len is None:\n","        src_len = 0\n","\n","        for item in dataset:\n","            src_ids = tokenizer.encode(item['Context']).ids\n","            src_len = max(src_len, len(src_ids))\n","\n","        print(f'Max length of source:', src_len)\n","        src_len += 2 # [SOS] and [EOS] tokens\n","    else:\n","        src_len = src_seq_len\n","\n","    if tgt_seq_len is None:\n","        tgt_len = 0\n","\n","        for item in dataset:\n","            tgt_ids = tokenizer.encode(item['Response']).ids\n","            tgt_len = max(tgt_len, len(tgt_ids))\n","\n","        print(f'Max length of target:', tgt_len)\n","        tgt_len += 1 # [EOS] token\n","    else:\n","        tgt_len = tgt_seq_len\n","\n","\n","    train_ds = MHDataset(train_ds, tokenizer, src_seq_len, tgt_seq_len)\n","    val_ds = MHDataset(val_ds, tokenizer, src_seq_len, tgt_seq_len)\n","\n","\n","\n","    train_dataloader = DataLoader(train_ds, batch_size=train_batch_size, shuffle=True)\n","    val_dataloader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=True)\n","\n","    \n","    return train_dataloader, val_dataloader, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"_LUf7QdudzMA"},"source":["# Validation loop"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:03.923854Z","iopub.status.busy":"2024-06-10T13:53:03.923368Z","iopub.status.idle":"2024-06-10T13:53:08.799861Z","shell.execute_reply":"2024-06-10T13:53:08.798934Z","shell.execute_reply.started":"2024-06-10T13:53:03.923827Z"},"id":"XPij5OJddzMA","trusted":true},"outputs":[],"source":["def run_validation(model:Transformer, loss_fn, validation_loader:DataLoader, tokenizer:Tokenizer, device:torch.device, writer:SummaryWriter, global_step:int) -> None:\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        batch_iterator = tqdm(validation_loader, desc=f\"Validation step\")\n","\n","        for id, batch in enumerate(batch_iterator):\n","            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n","            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n","            # print(decoder_input.shape)\n","            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n","            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n","\n","\n","            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n","            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n","            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n","\n","            \n","            label = batch['label'].to(device) # (B, seq_len)\n","\n","            # Compute the loss using a simple cross entropy\n","            loss = loss_fn(proj_output.view(-1, tokenizer.get_vocab_size()), label.view(-1))\n","            \n","            \n","            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n","            writer.add_scalar('val loss', loss, global_step)\n","            writer.flush()    \n"]},{"cell_type":"markdown","metadata":{"id":"DZrlE1AadzMB"},"source":["# Training"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:08.801676Z","iopub.status.busy":"2024-06-10T13:53:08.801077Z","iopub.status.idle":"2024-06-10T13:53:08.819029Z","shell.execute_reply":"2024-06-10T13:53:08.817882Z","shell.execute_reply.started":"2024-06-10T13:53:08.801628Z"},"id":"AJU_wlFRdzMB","trusted":true},"outputs":[],"source":["\n","# model_preload - dict, keys: ('model_state_dict', 'global_step', 'optimizer_state_dict)\n","\n","def train_model(model:Transformer, num_epochs:int, device:torch.device, train_loader:DataLoader, val_loader:DataLoader, tokenizer:Tokenizer, loss_fn, optimizer:torch.optim, writer:SummaryWriter, model_preload:str, save_every_epoch: bool, train_decode_step: int = None):\n","    print('Using device:', device)\n","\n","    global_step = 0\n","\n","    if model_preload:\n","        print(f'Preloading model {model_preload}')\n","        state = torch.load(model_preload)\n","        global_step = model_preload['global_step']\n","        model.load_state_dict(state['model_state_dict'])\n","        optimizer.load_state_dict(state['optimizer_state_dict'])\n","\n","    for epoch in range(num_epochs):\n","        torch.cuda.empty_cache()\n","        model.train()\n","\n","        batch_iterator = tqdm(train_loader, desc=f\"Processing Epoch {epoch:02d}\")\n","\n","        for id, batch in enumerate(batch_iterator):\n","\n","            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n","            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n","            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n","            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n","\n","            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n","            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n","            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n","\n","            label = batch['label'].to(device) # (B, seq_len)\n","       \n","            loss = loss_fn(proj_output.view(-1, tokenizer.get_vocab_size()), label.view(-1))\n","            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n","\n","\n","\n","            writer.add_scalar('train loss', loss.item(), global_step)\n","            writer.flush()\n","\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            if train_decode_step is not None:\n","                \n","                if id % train_decode_step == 0:\n","                    source_text = batch['src_text'][0]\n","                    truth_text = batch['tgt_text'][0]\n","\n","                    pred_tokens = torch.argmax(proj_output, dim=-1)[0]\n","                    pred_text = tokenizer.decode(pred_tokens.tolist())\n","                    print('\\n')\n","                    print('*' * 80)\n","                    print('Source: ', source_text)\n","                    print('Truth: ', truth_text)\n","                    print('Pred: ', pred_text)\n","\n","        global_step += 1\n","        run_validation(model, val_loader, tokenizer, device, writer, global_step)\n","\n","        if save_every_epoch:\n","            save_model(f'model_gep_{global_step}.pt', model, optimizer, global_step)\n","\n","\n","\n","def save_model(filename, model, optimizer, global_step):\n","    try:\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'global_step': global_step\n","        }, filename)\n","\n","    except Exception as e:\n","        print(str(e))\n","        pass\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8TDDN955dzMB"},"source":["# Training the model!"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261,"referenced_widgets":["eac68c03c0f14cef9453e443bb7f1eb0","d11288ee69ae4e67b0441fb8200192a3","f48a94e849bc4acdb4c644c7aa92c0b5","c193c3e58ddc4bec995e3ebf5d701871","70de4fcf81184929a53a2c5d6f6240b2","7621615088bc4e74896441778353d4c5","29cefed6d90246468b39891c4172642a","957940a00d2e4193a936f3d40726909f","a251600667774f909cb25010d13c3ba8","4db89227d46f4021b3fe25a008e7c165","e94270fc79b748a5b8c730f25ec44539","65586c6496694c829e2dca6f994da301","826e695db425459aac87cea3db3f7a71","7110c2288311473f8bc78be4a046fb0f","03e54a919a2d4fb0a32af83afbea1317","166ef09b586d494b923234208067e77c","843974be004e4b96a6e65929a48de001","1d9e50616f42487a867ded907fdf510d","661802496e55433495bb44ce96948810","1a337358a97a46dd8eb657c99f5c8f6f","8ae2cf83026a4a3592c4e90b97c086e6","83697e8563834c6abb6a7bee14e965ad","a1b3c1cd9caf4db29d4f9e44c843e107","aea7a5b381264edd8d299802274d3d22","31f94008299c44b0913df68e01acffcc","3ba3faf11fd642bb99b68dee4f0be565","6ba07d9cbf8e4537a80b614e788a4911","750d0d72d9594795afbc5857118b4dce","4498d5568f3f48549878530adbbe362a","90c80f4b6500468ea1fb187b3e2ef552","07c69299d43844be88de0679f29c6b99","370975a1e3694d4b829dbb2215ca5fed","19b3055b8efa4ffeac51ad48c2e0c441"]},"execution":{"iopub.execute_input":"2024-06-10T13:53:08.820951Z","iopub.status.busy":"2024-06-10T13:53:08.820518Z","iopub.status.idle":"2024-06-10T13:53:24.048314Z","shell.execute_reply":"2024-06-10T13:53:24.047180Z","shell.execute_reply.started":"2024-06-10T13:53:08.820912Z"},"id":"WEeD_aFVdzMB","outputId":"dff28e14-b077-4805-a78c-3a0188128625","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fetching dataset\n","Dataset loaded successfully!\n","Creating train-vali split\n","Training samples: 4475\n","Validation samples 498\n"]}],"source":["MAX_SRC_LEN = 150\n","MAX_TGT_LEN = 451\n","\n","\n","train_dataloader, val_dataloader, tokenizer = prepare_dataset(train_size=0.9, tgt_seq_len=MAX_TGT_LEN, src_seq_len=MAX_SRC_LEN, train_batch_size = 8)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:24.050945Z","iopub.status.busy":"2024-06-10T13:53:24.049900Z","iopub.status.idle":"2024-06-10T13:53:24.061754Z","shell.execute_reply":"2024-06-10T13:53:24.060511Z","shell.execute_reply.started":"2024-06-10T13:53:24.050902Z"},"id":"5Ag0IzZYdzMC","trusted":true},"outputs":[],"source":["src_vocab_size = tokenizer.get_vocab_size()\n","tgt_vocab_size = tokenizer.get_vocab_size()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T13:53:24.064130Z","iopub.status.busy":"2024-06-10T13:53:24.063436Z","iopub.status.idle":"2024-06-10T13:53:24.708062Z","shell.execute_reply":"2024-06-10T13:53:24.707156Z","shell.execute_reply.started":"2024-06-10T13:53:24.064090Z"},"id":"YfnENKxadzMC","trusted":true},"outputs":[],"source":["transformer = build_transformer(N = 4, d_model = 256, h = 8, src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size, src_seq_len=MAX_SRC_LEN, tgt_seq_len=MAX_TGT_LEN)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-10T13:53:24.709673Z","iopub.status.busy":"2024-06-10T13:53:24.709335Z","iopub.status.idle":"2024-06-10T13:53:24.719034Z","shell.execute_reply":"2024-06-10T13:53:24.718050Z","shell.execute_reply.started":"2024-06-10T13:53:24.709643Z"},"id":"E4i5U4CMqGSZ","outputId":"dea8e859-6d18-47b2-c066-8282527e28f6","trusted":true},"outputs":[{"data":{"text/plain":["Transformer(\n","  (encoder): Encoder(\n","    (layers): ModuleList(\n","      (0-3): 4 x EncoderBlock(\n","        (self_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=256, out_features=256, bias=False)\n","          (w_k): Linear(in_features=256, out_features=256, bias=False)\n","          (w_v): Linear(in_features=256, out_features=256, bias=False)\n","          (w_o): Linear(in_features=256, out_features=256, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward_block): FeedForwardBlock(\n","          (linear_1): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear_2): Linear(in_features=2048, out_features=256, bias=True)\n","        )\n","        (residual_connections): ModuleList(\n","          (0-1): 2 x ResidualConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNormalization()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNormalization()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0-3): 4 x DecoderBlock(\n","        (self_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=256, out_features=256, bias=False)\n","          (w_k): Linear(in_features=256, out_features=256, bias=False)\n","          (w_v): Linear(in_features=256, out_features=256, bias=False)\n","          (w_o): Linear(in_features=256, out_features=256, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (cross_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=256, out_features=256, bias=False)\n","          (w_k): Linear(in_features=256, out_features=256, bias=False)\n","          (w_v): Linear(in_features=256, out_features=256, bias=False)\n","          (w_o): Linear(in_features=256, out_features=256, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward_block): FeedForwardBlock(\n","          (linear_1): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear_2): Linear(in_features=2048, out_features=256, bias=True)\n","        )\n","        (residual_connections): ModuleList(\n","          (0-2): 3 x ResidualConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNormalization()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNormalization()\n","  )\n","  (src_embed): InputEmbeddings(\n","    (embedding): Embedding(20192, 256)\n","  )\n","  (tgt_embed): InputEmbeddings(\n","    (embedding): Embedding(20192, 256)\n","  )\n","  (src_pos): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (tgt_pos): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (projection_layer): ProjectionLayer(\n","    (proj): Linear(in_features=256, out_features=20192, bias=True)\n","  )\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%tensorboard --logdir runs"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2024-06-10T13:53:24.724070Z","iopub.status.busy":"2024-06-10T13:53:24.723750Z"},"id":"pVxt2JzQdzMD","outputId":"b8d931ea-2e4a-4263-eda4-2fe92b51639c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 00:   0%|          | 1/560 [00:05<51:08,  5.49s/it, loss=9.939]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","********************************************************************************\n","Source:  I was the one who ended it, and I'm so glad I did. It was the best decision I made in my life. But how do I stop the nightmares and flashbacks? It is creating a wall in my current relationship.\n","Truth:  From what you describe about yourself, I agree with you that ending your former relationship was a very wise decision.The nightmares and flashbacks show that you were deeply affected emotionally and on the foundations of your basic nature.The way for these to stop is by the slow process of realizing how badly injured and frightened you were of your former partner.Once you've stabilized yourself by accepting the tremendous harshness that was part of the former relationship, then the nightmares and flashbacks will disappear gradually usually, maybe all at once.There is a possibility too that your former relationship connected with being emotionally ignored, abandoned, treated harshly during your time of growing up years.Since generally people choose partners who relate similarly to the ways in which they felt treated by parents, it is possible that you had been badly treated while growing up and weren't aware of this until going through this terrible relationship.Congratulations on ending your relationship!\n","Pred:  distorted distorted Jack moms) refocus, diminishing follow etc? diminishing moms) diminishing diminishing ex-girlfriend permitiéndoles diminishing unemployed unemployed unemployed permitiéndoles well). permitiéndoles well). red, diminishing is...I'm mouth, (SAD) mouth, ex-girlfriend mouth, marvelous mouth, normal.Hopefully diminishing red, heh, well). anti-depressants?This distorted mouth, mouth, marvelous mouth, diminishing baggy refocus, distorted refocus, mouth, way.Part heh, diminishing Paul heh, heh, distorted well). confidante mouth, diminishing mouth, heh, stressed.Imagine confidante mouth, slightly kinks moms) is...I'm mouth, marvelous heh, mouth, unworthy, mouth, is...I'm heh, erase heh, unworthy, way.Part mouth, permitiéndoles heh, us.Sometimes waited. mouth, confidante kinks anti-depressants?This diminishing distorted Woman mouth, is...I'm mouth, mouth, refocus, diminishing anti-depressants?This mouth, control/manage is...I'm mouth, mouth, mouth, unworthy, is...I'm mouth, mouth, mouth, mouth, unworthy, unworthy, is...I'm mouth, heh, mouth, mouth, mouth, mouth, mouth, mouth, mouth, mouth, mouth, confidante mouth, mouth, mouth, Woman daughter’s mouth, mouth, mouth, mouth, mouth, mouth, mouth, mouth, mouth, heh, mouth, mouth, confidante confidante mouth, unworthy, mouth, mouth, mouth, mouth, mouth, mouth, mouth, distorted mouth, mouth, mouth, mouth, mouth, confidante mouth, mouth, mouth, finger-pointing, mouth, diminishing mouth, mouth, mouth, mouth, mouth, anti-depressants?This mouth, mouth, mouth, mouth, Christmas. marvelous mouth, mouth, mouth, mouth, heh, Christmas. mouth, mouth, through is...I'm unworthy, mouth, encouragement mouth, mouth, mouth, mouth, mouth, heh, mouth, confidante unworthy, mouth, confidante anti-depressants?This heh, mouth, mouth, way.Part mouth, mouth, confidante mouth, diminishing mouth, mouth, mouth, is...I'm control/manage mouth, mouth, affiliation. expertise, stuff. short-acting. anti-depressants?This ENJOY level mouth, refocus, distorted distorted mouth, distorted heh, Christmas. Christmas. Christmas. Herman, proveedores skilled mouth, control/manage mouth, erase affiliation. mouth, mouth, mouth, mouth, marvelous mouth, mouth, down\" mouth, mouth, erase confidante mouth, heh, mouth, mouth, mouth, kinks mouth, way.Part mouth, Christmas. baggy mouth, mouth, confidante Christmas. Christmas. skilled skilled mouth, mouth, expertise, heh, expertise, opinions unworthy, Christmas. mouth, confidante distorted mouth, erase unworthy, Subway heh, heh, Christmas. heh, confidante confidante erase heh, Christmas. heh, baggy mouth, is...I'm mouth, distorted Christmas. anti-depressants?This mouth, Christmas. Christmas. mouth, mouth, distorted mouth, marvelous mouth, Christmas. Christmas. confidante Christmas. distorted heh, unworthy, distorted distorted distorted mouth, chance, mouth, mouth, distorted distorted distorted skilled anti-depressants?This chance, Jack chance, mouth, chance, confidante Jack mouth, oral unworthy, mouth, step-father is...I'm mouth, Christmas. chance, mouth, unworthy, baggy mouth, Woman mouth, mouth, mouth, mouth, chance, mouth, chance, mouth, unworthy, chance, mouth, Christmas. unworthy, minutos sounding. mouth, unworthy, Christmas. unworthy, Christmas. expertise, right?Maybe mouth, unworthy, mouth, heh, distorted mouth, skilled Christmas. mouth, distorted ex-girlfriend mouth, is...I'm skilled confidante control/manage mouth, mouth, mouth, heh, mouth, Christmas. explode. mouth, confidante mouth, through confidante distorted Christmas. mouth, Christmas. diminishing distorted mouth, mouth, mouth, ex-girlfriend mouth, mouth, mouth, heh, mouth, mouth, mouth, distorted distorted Christmas. mouth, mouth, mouth, elsewhere unworthy, distorted mouth, mouth, heh, Christmas. mouth, marvelous mouth, mouth, down\" Christmas. Christmas. Jack mouth, mouth, Finch, expertise, step-father Harbor\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 00:   2%|▎         | 14/560 [01:24<54:46,  6.02s/it, loss=7.631]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/mhmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m transformer \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_preload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_decode_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, device, train_loader, val_loader, tokenizer, loss_fn, optimizer, writer, model_preload, save_every_epoch, train_decode_step)\u001b[0m\n\u001b[1;32m     25\u001b[0m encoder_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, 1, 1, seq_len)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m decoder_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, 1, seq_len, seq_len)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(encoder_output, encoder_mask, decoder_input, decoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(decoder_output) \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 277\u001b[0m, in \u001b[0;36mTransformer.encode\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m    275\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(src)\n\u001b[1;32m    276\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_pos(src)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 218\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 218\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 206\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, src_mask)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[0;32m--> 206\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sublayer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mLayerNormalization.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m mean \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# (batch, seq_len, 1)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Keep the dimension for broadcasting\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch, seq_len, 1)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# eps is to prevent dividing by zero or when std is very small\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m (std \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["num_epochs = 20\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","torch.cuda.empty_cache()\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3)\n","\n","writer = SummaryWriter('runs/mhmodel')\n","\n","\n","transformer = transformer.to(device)\n","\n","\n","train_model(model = transformer, num_epochs=num_epochs, device=device, train_loader=train_dataloader, val_loader=val_dataloader,\n","tokenizer = tokenizer, loss_fn=loss_fn, optimizer=optimizer, writer = writer, model_preload=None, save_every_epoch=True, train_decode_step=100)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.-1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03e54a919a2d4fb0a32af83afbea1317":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ae2cf83026a4a3592c4e90b97c086e6","placeholder":"​","style":"IPY_MODEL_83697e8563834c6abb6a7bee14e965ad","value":" 4.79M/4.79M [00:00&lt;00:00, 15.4MB/s]"}},"07c69299d43844be88de0679f29c6b99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"166ef09b586d494b923234208067e77c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19b3055b8efa4ffeac51ad48c2e0c441":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a337358a97a46dd8eb657c99f5c8f6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d9e50616f42487a867ded907fdf510d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29cefed6d90246468b39891c4172642a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31f94008299c44b0913df68e01acffcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_90c80f4b6500468ea1fb187b3e2ef552","max":3512,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07c69299d43844be88de0679f29c6b99","value":3512}},"370975a1e3694d4b829dbb2215ca5fed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba3faf11fd642bb99b68dee4f0be565":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_370975a1e3694d4b829dbb2215ca5fed","placeholder":"​","style":"IPY_MODEL_19b3055b8efa4ffeac51ad48c2e0c441","value":" 3512/3512 [00:00&lt;00:00, 35058.18 examples/s]"}},"4498d5568f3f48549878530adbbe362a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4db89227d46f4021b3fe25a008e7c165":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65586c6496694c829e2dca6f994da301":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_826e695db425459aac87cea3db3f7a71","IPY_MODEL_7110c2288311473f8bc78be4a046fb0f","IPY_MODEL_03e54a919a2d4fb0a32af83afbea1317"],"layout":"IPY_MODEL_166ef09b586d494b923234208067e77c"}},"661802496e55433495bb44ce96948810":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ba07d9cbf8e4537a80b614e788a4911":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70de4fcf81184929a53a2c5d6f6240b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7110c2288311473f8bc78be4a046fb0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_661802496e55433495bb44ce96948810","max":4790520,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a337358a97a46dd8eb657c99f5c8f6f","value":4790520}},"750d0d72d9594795afbc5857118b4dce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7621615088bc4e74896441778353d4c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"826e695db425459aac87cea3db3f7a71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_843974be004e4b96a6e65929a48de001","placeholder":"​","style":"IPY_MODEL_1d9e50616f42487a867ded907fdf510d","value":"Downloading data: 100%"}},"83697e8563834c6abb6a7bee14e965ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"843974be004e4b96a6e65929a48de001":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ae2cf83026a4a3592c4e90b97c086e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90c80f4b6500468ea1fb187b3e2ef552":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"957940a00d2e4193a936f3d40726909f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1b3c1cd9caf4db29d4f9e44c843e107":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aea7a5b381264edd8d299802274d3d22","IPY_MODEL_31f94008299c44b0913df68e01acffcc","IPY_MODEL_3ba3faf11fd642bb99b68dee4f0be565"],"layout":"IPY_MODEL_6ba07d9cbf8e4537a80b614e788a4911"}},"a251600667774f909cb25010d13c3ba8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aea7a5b381264edd8d299802274d3d22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_750d0d72d9594795afbc5857118b4dce","placeholder":"​","style":"IPY_MODEL_4498d5568f3f48549878530adbbe362a","value":"Generating train split: 100%"}},"c193c3e58ddc4bec995e3ebf5d701871":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4db89227d46f4021b3fe25a008e7c165","placeholder":"​","style":"IPY_MODEL_e94270fc79b748a5b8c730f25ec44539","value":" 2.82k/2.82k [00:00&lt;00:00, 198kB/s]"}},"d11288ee69ae4e67b0441fb8200192a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7621615088bc4e74896441778353d4c5","placeholder":"​","style":"IPY_MODEL_29cefed6d90246468b39891c4172642a","value":"Downloading readme: 100%"}},"e94270fc79b748a5b8c730f25ec44539":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eac68c03c0f14cef9453e443bb7f1eb0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d11288ee69ae4e67b0441fb8200192a3","IPY_MODEL_f48a94e849bc4acdb4c644c7aa92c0b5","IPY_MODEL_c193c3e58ddc4bec995e3ebf5d701871"],"layout":"IPY_MODEL_70de4fcf81184929a53a2c5d6f6240b2"}},"f48a94e849bc4acdb4c644c7aa92c0b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_957940a00d2e4193a936f3d40726909f","max":2816,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a251600667774f909cb25010d13c3ba8","value":2816}}}}},"nbformat":4,"nbformat_minor":4}
