{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title-block-banner: true\n",
        "title: Mental health transformer counselor\n",
        "subtitle: Model evaluation\n",
        "author:\n",
        "  - name: Piotr Szyszka\n",
        "    email: piotrulo022@gmail.com\n",
        "    affiliations:\n",
        "      - name: Lublin University of Technology\n",
        "        department: Faculty of Mathematics and Information Technology\n",
        "        url: 'https://pollub.pl/en/'\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    self-contained: true\n",
        "    toc: true\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is focused to validate and evaluate trained model that imitates mental health counselor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Necessary imports\n",
        "from modules.model import *\n",
        "from modules.dataset_utils import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "\n",
        "from torchmetrics.text import Perplexity, BLEUScore\n",
        "from jiwer import wer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "weights = torch.load('/home/piotr/Downloads/model_15-06.pt', map_location=torch.device('cpu'))\n",
        "\n",
        "tokenizer = weights['tokenizer']\n",
        "src_vocab_size = tokenizer.get_vocab_size()\n",
        "tgt_vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "MAX_SRC_LEN = 90 \n",
        "MAX_TGT_LEN = 300\n",
        "\n",
        "model = build_transformer(N = 4, d_model = 512, h = 16, src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size, src_seq_len=MAX_SRC_LEN, tgt_seq_len=MAX_TGT_LEN)\n",
        "\n",
        "model.load_state_dict(weights['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "dataset = DS.from_parquet('./MH_test.parquet')\n",
        "test_ds = MHDataset(dataset, tokenizer, MAX_SRC_LEN, MAX_TGT_LEN)\n",
        "test_loader = DataLoader(test_ds, batch_size = 16, shuffle = True)\n",
        "test_dec_loader = DataLoader(test_ds, batch_size = 1, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human rating\n",
        "\n",
        "While evaluation metrics are crucial for assessing large language models (LLMs), the ultimate measure of their value lies in their practical usefulness for specific tasks. Metrics can provide a quantitative snapshot of performance, but they may not capture all nuances (especially for such task like this). Therefore, it's essential to also manually review and analyze the results to ensure they meet the real-world needs and expectations of users.\n",
        "\n",
        "Let's generate some of responses using *greedy decoding* or *beam search* method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer, max_len, device):\n",
        "    sos_idx = tokenizer.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        deco_mask = decoder_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, deco_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "    \n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "def beam_search_decode(model, beam_size, source, source_mask, tokenizer, max_len, device):\n",
        "    sos_idx = tokenizer.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    # Create a candidate list\n",
        "    candidates = [(decoder_initial_input, 1)]\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
        "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
        "            break\n",
        "\n",
        "        # Create a new list of candidates\n",
        "        new_candidates = []\n",
        "\n",
        "        for candidate, score in candidates:\n",
        "\n",
        "            # Do not expand candidates that have reached the eos token\n",
        "            if candidate[0][-1].item() == eos_idx:\n",
        "                continue\n",
        "\n",
        "            # Build the candidate's mask\n",
        "            candidate_mask = decoder_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
        "            # calculate output\n",
        "            with torch.no_grad():\n",
        "                out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
        "                # get next token probabilities\n",
        "                prob = model.project(out[:, -1])\n",
        "                # get the top k candidates\n",
        "                topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
        "                for i in range(beam_size):\n",
        "                    # for each of the top k candidates, get the token and its probability\n",
        "                    token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
        "                    token_prob = topk_prob[0][i].item()\n",
        "                    # create a new candidate by appending the token to the current candidate\n",
        "                    new_candidate = torch.cat([candidate, token], dim=1)\n",
        "                    # We sum the log probabilities because the probabilities are in log space\n",
        "                    new_candidates.append((new_candidate, score + token_prob))\n",
        "\n",
        "        # Sort the new candidates by their score\n",
        "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "        # Keep only the top k candidates\n",
        "        candidates = candidates[:beam_size]\n",
        "\n",
        "        # If all the candidates have reached the eos token, stop\n",
        "        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
        "            break\n",
        "\n",
        "    # Return the best candidate\n",
        "    return candidates[0][0].squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def prepare_input(input_text, tokenizer):\n",
        "    sos_token = torch.tensor([tokenizer.token_to_id('[SOS]')], dtype = torch.int64)\n",
        "    eos_token = torch.tensor([tokenizer.token_to_id('[EOS]')], dtype = torch.int64)\n",
        "    pad_token = torch.tensor([tokenizer.token_to_id('[PAD]')], dtype = torch.int64)\n",
        "    \n",
        "    \n",
        "    # prepare inputs for encoder\n",
        "    input_tokens = tokenizer.encode(input_text).ids\n",
        "    \n",
        "    padding_len = MAX_SRC_LEN - len(input_tokens) - 2 # [SOS] and [EOS]\n",
        "\n",
        "    if padding_len <= 0:\n",
        "        # truncate\n",
        "        input_tokens = input_tokens[:MAX_SRC_LEN - 2]\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                sos_token,\n",
        "                torch.tensor(input_tokens, dtype = torch.int64),\n",
        "                eos_token\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                sos_token,\n",
        "                torch.tensor(input_tokens, dtype = torch.int64),\n",
        "                eos_token,\n",
        "                torch.tensor([pad_token] * padding_len, dtype = torch.int64)\n",
        "            ]\n",
        "        )\n",
        "    encoder_input = encoder_input.unsqueeze(0) # add batch dim\n",
        "    # create encoder mask\n",
        "    encoder_mask = (encoder_input != pad_token).unsqueeze(0).unsqueeze(0).int()\n",
        "\n",
        "    return {'encoder_input': encoder_input,\n",
        "            'encoder_mask': encoder_mask}\n",
        "\n",
        "\n",
        "def generate_text(input_text: str, model: Transformer, tokenizer: Tokenizer, output_len: int = 50, method: str = 'greedy', device: str = 'cpu'):\n",
        "\n",
        "    encoder_ins = prepare_input(input_text, tokenizer)\n",
        "    encoder_input, encoder_mask = encoder_ins['encoder_input'], encoder_ins['encoder_mask']\n",
        "    if method == 'greedy':\n",
        "        model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer, output_len, device)\n",
        "    else:\n",
        "        model_out = beam_search_decode(model, 3, encoder_input, encoder_mask, tokenizer, output_len, device)\n",
        "\n",
        "    model_out_text = tokenizer.decode(model_out.tolist())\n",
        "    \n",
        "    return model_out_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "greetings = \"Hello there! How are you?\"\n",
        "greetings_response = generate_text(greetings, model = model, method = 'greedy', tokenizer=tokenizer, output_len=30, device = 'cpu')\n",
        "\n",
        "intro = \"What is your name? Tell me something about yourself.\"\n",
        "intro_response = generate_text(intro, model = model, tokenizer=tokenizer, output_len=30, device = 'cpu')\n",
        "\n",
        "\n",
        "sadge = \"I have problems with my boss in my job. He gives me too many task and I'm overwhelmed by work.\"\n",
        "\n",
        "sadge_response = generate_text(sadge, model = model, method = 'greedy', tokenizer=tokenizer, output_len=30, device = 'cpu')\n",
        "\n",
        "\n",
        "sadge2 = \"I don't know what I want to do in my life\"\n",
        "sadge2_response = generate_text(sadge2, model = model, method = 'greedy', tokenizer=tokenizer, output_len=30, device = 'cpu')\n",
        "\n",
        "\n",
        "print('*'* 40, 'GREETINGS', '*'*40)\n",
        "print('INPUT:', greetings)\n",
        "print('OUTPUT:', greetings_response)\n",
        "\n",
        "print('*'* 40, 'INTRO', '*'* 40)\n",
        "print('INPUT:', intro)\n",
        "print('OUTPUT:', intro_response)\n",
        "\n",
        "print('*'* 40, 'SADGE', '*'* 40)\n",
        "print('INPUT:', sadge)\n",
        "print('OUTPUT:', sadge_response)\n",
        "\n",
        "print('*'* 40, 'SADGE2', '*'* 40)\n",
        "print('INPUT:', sadge2)\n",
        "print('OUTPUT:', sadge2_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decode test dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def decode_batches(model, validation_ds, tokenizer, max_len, device, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "\n",
        "            # model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer, max_len, device)\n",
        "            model_out = beam_search_decode(model, 2, encoder_input, encoder_mask, tokenizer, max_len, device)\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "            \n",
        "            # Print the source, target and model output\n",
        "            print('-'*80)\n",
        "            print(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print('-'*80)\n",
        "                break\n",
        "\n",
        "\n",
        "decode_batches(model, test_dec_loader, tokenizer, 20, 'cpu', 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The responses generated by the model are quite satisfying. They maintain a strong alignment with the subject matter of the inputs, ensuring relevance. Additionally, the sentences are well-constructed, and the lengths of the outputs are appropriate, providing a balanced and coherent response.\n",
        "\n",
        "# Evaluation on test dataset\n",
        "\n",
        "It is finally time to evaluate metrics on test dataset.\n",
        "\n",
        "Finding a valid metrics for this task is not so easy. We will observe:\n",
        "\n",
        "- `perplexity`- measures how well a language model predicts a text sample. It’s calculated as the average number of bits per word a model needs to represent the sample. Lower average bits per word implies better performance in the context of predicting text,\n",
        "\n",
        "- `word error rate` - common metric of the performance of an automatic speech recognition system. This value indicates the percentage of words that were incorrectly predicted. The lower the value, the better the performance of the ASR system with a WER of 0 being a perfect score,\n",
        "\n",
        "- `token accuracy` - token accuracy measures the percentage of tokens (words or subwords) that the model correctly predicts in a given text sample. Higher token accuracy indicates higher performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ppx = Perplexity(ignore_index=tokenizer.token_to_id('[PAD]'))\n",
        "\n",
        "def evaluate_metrics(model:Transformer, loss_fn, test_loader:DataLoader, tokenizer:Tokenizer, device:torch.device) -> None:\n",
        "    model.eval()\n",
        "\n",
        "    wers = []\n",
        "    token_accuracies = []\n",
        "    pers = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_iterator = tqdm(test_loader, desc=f\"Validation step\")\n",
        "\n",
        "        for id, batch in enumerate(batch_iterator):\n",
        "            encoder_input = batch['encoder_input'].to(device) \n",
        "            decoder_input = batch['decoder_input'].to(device) \n",
        "\n",
        "            encoder_mask = batch['encoder_mask'].to(device) \n",
        "            decoder_mask = batch['decoder_mask'].to(device) \n",
        "\n",
        "            label = batch['label'].to(device) # ground_truth\n",
        "\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output) # logits\n",
        "            \n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer.get_vocab_size()), label.view(-1))\n",
        "            \n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            \n",
        "            per = ppx(proj_output, label)\n",
        "            pers.append(per)\n",
        "\n",
        "            pred_tokens = torch.argmax(proj_output, -1)\n",
        "            for preds, target in zip(pred_tokens, label):\n",
        "                pred_text = tokenizer.decode(preds.tolist())\n",
        "                target_text = tokenizer.decode(target.tolist())\n",
        "                \n",
        "                # 1) Token accuracy:\n",
        "                correct_predictions = (preds == target).sum().item()\n",
        "                total_tokens = len(target)\n",
        "                sentence_accuracy = correct_predictions / total_tokens\n",
        "                token_accuracies.append(sentence_accuracy)\n",
        "                \n",
        "                # 2) WER\n",
        "                try:\n",
        "                    wers.append(wer(target_text, pred_text))\n",
        "                except ValueError:\n",
        "                    pass\n",
        "    return {'Token_accuracy': torch.tensor(token_accuracies, dtype = float).mean(),\n",
        "            'Perplexity': torch.tensor(pers, dtype = float).mean(),\n",
        "            'WER': torch.tensor(wers, dtype = float).mean(),\n",
        "            'Loss': torch.tensor(losses, dtype = float).mean()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "device = 'cpu'\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "evaluate_metrics(model, loss_fn = loss_fn, test_loader=test_loader, tokenizer=tokenizer, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Token accuracy of 0.9032 means that the model correctly predicts about 90.32% of the tokens (words or subwords) it generates. This is a relatively high accuracy, indicating that the model is quite effective at predicting the correct tokens.\n",
        "But there is also a dark side of that. While token accuracy is high, it doesn’t account for the overall context or semantic meaning of the responses. In mental health counseling, understanding and generating contextually and emotionally appropriate responses are crucial, which may not always be captured by token accuracy alone.\n",
        "\n",
        "A perplexity of 85.1507 indicates that, on average, the model considers 85.1507 different possibilities for the next token. Lower perplexity values are preferred as they indicate more confidence and precision in predictions. However, perplexity values around 85 for a relatively small dataset suggest that the model is still learning and has room for improvement. High perplexity might mean the model struggles with generating contextually coherent responses. In the context of mental health, this could lead to responses that are less tailored or specific to the nuances of a patient's input.\n",
        "\n",
        "A WER of 0.6263 means that 62.63% of the words in the generated response are different from the expected response. In our case it is quite high, suggesting that more than half of the model's output is different from the reference.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Considering the relatively small training dataset, the model's performance is promising, but for practical deployment in mental health counseling, additional training with more data and fine-tuning might be necessary to enhance its contextual and semantic capabilities.\n",
        "\n",
        "# Vizualization of attention layers\n",
        "\n",
        "Visualizing attention layers of an encoder helps understand model behavior, debug issues, and verify learning. It provides insights into how the model processes information, identifies where it focuses during predictions, and aids in diagnosing errors or biases in its decision-making. This transparency enhances interpretability and trustworthiness of the model's outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_attention_scores(input_text, model: Transformer, tokenizer, layer: int, head: int):\n",
        "    generate_text(input_text, model, tokenizer, output_len=2, device= 'cpu')\n",
        "\n",
        "    decoder_layer = model.decoder.layers[layer] # type() = EncoderBlock\n",
        "    encoder_layer = model.encoder.layers[layer] # type() = EncoderBlock\n",
        "    encoder_attention = encoder_layer.self_attention_block.attention_scores\n",
        "\n",
        "    cross_attention = decoder_layer.cross_attention_block.attention_scores\n",
        "\n",
        "    return {'encoder_attention_scores': encoder_attention[:, head, :, :],\n",
        "            'cross_attention_scores': cross_attention[:, head, :, :]}\n",
        "\n",
        "\n",
        "def plot_attention(input_text, model, tokenizer):\n",
        "    def plot_layer_head(layer, head):\n",
        "        attention = get_attention_scores(input_text, model, tokenizer, layer, head)\n",
        "        input_len = len(input_text.split()) + 1\n",
        "        attention_encoder = attention['encoder_attention_scores'].squeeze().detach().cpu().numpy()\n",
        "        input_tokens = prepare_input(input_text, tokenizer)['encoder_input']\n",
        "        input_tokens = input_tokens[0, :input_len]\n",
        "        decoded_tokens = [tokenizer.id_to_token(tok) for tok in input_tokens]\n",
        "\n",
        "        attention_encoder = attention_encoder[:input_len, :input_len]\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(attention_encoder, xticklabels=decoded_tokens, yticklabels=decoded_tokens,\n",
        "                    cmap='coolwarm', fmt=\".2f\", cbar_kws={\"shrink\": .8})\n",
        "        \n",
        "        plt.title(f'Attention Scores for Layer {layer}, Head {head}')\n",
        "        plt.xlabel('Tokens')\n",
        "        plt.ylabel('Tokens')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.show()\n",
        "\n",
        "    num_layers = 4\n",
        "    num_heads = 16\n",
        "\n",
        "    layer_widget = widgets.IntSlider(min=0, max=num_layers-1, value=0, description='Layer:')\n",
        "    head_widget = widgets.IntSlider(min=0, max=num_heads-1, value=0, description='Head:')\n",
        "    \n",
        "    widgets.interact(plot_layer_head, layer=layer_widget, head=head_widget)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_attention(\"I'm having trouble with my romantic relationship, what can I do?\", model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions\n",
        "\n",
        "Goals of project have been achieved. Although built model is not perfect, it gives quite satisfying responses related to topic. \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}