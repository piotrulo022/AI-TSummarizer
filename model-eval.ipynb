{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.token_to_id('[SOS]')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda'\n",
    "# Define the top-k sampling function\n",
    "def top_k_sampling(logits: torch.Tensor, k: int, temperature: float = 1.0) -> int:\n",
    "    \"\"\"\n",
    "    Perform top-k sampling to select the next token.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The logits (unnormalized predictions) from the model of shape (vocab_size,).\n",
    "        k (int): The number of top tokens to consider for sampling.\n",
    "        temperature (float): The temperature value to scale logits. Higher temperature results in more randomness.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the selected token.\n",
    "    \"\"\"\n",
    "    # Scale logits by temperature\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Get the top-k logits and their indices\n",
    "    top_k_logits, top_k_indices = torch.topk(scaled_logits, k)\n",
    "    \n",
    "    # Re-normalize the top-k logits to get probabilities\n",
    "    probabilities = F.softmax(top_k_logits, dim=-1)\n",
    "    \n",
    "    # Sample from the top-k probabilities\n",
    "    next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "    \n",
    "    # Convert top-k index to original vocabulary index\n",
    "    print(top_k_indices[next_token])\n",
    "    return top_k_indices[next_token].item()\n",
    "\n",
    "# Sample input text\n",
    "input_text = \"Often times I find myself thinking scary thoughts and sometimes I even scare myself into thinking that something bad is going to happen to me. Once it starts, the thought continues going through my head and I can't get it out. How can I stop these thoughts?\"\n",
    "\n",
    "# Assuming tokenizer is already defined and initialized\n",
    "token_ids = tokenizer.encode(input_text).ids\n",
    "\n",
    "# Define special tokens and padding length\n",
    "MAX_SRC_LEN = 200  # Assuming a maximum source length of 200 for this example\n",
    "MAX_TGT_LEN = 100  # Assuming a maximum target length of 100 for the generated sequence\n",
    "\n",
    "sos_idx = tokenizer.token_to_id('[SOS]')\n",
    "eos_idx = tokenizer.token_to_id('[EOS]')\n",
    "pad_idx = tokenizer.token_to_id('[PAD]')\n",
    "\n",
    "# Compute the number of padding tokens required\n",
    "enc_num_pad = MAX_SRC_LEN - len(token_ids) - 2\n",
    "\n",
    "# Prepare the encoder input\n",
    "encoder_input = torch.cat(\n",
    "    [\n",
    "        torch.tensor([sos_idx], dtype=torch.int64),\n",
    "        torch.tensor(token_ids, dtype=torch.int64),\n",
    "        torch.tensor([eos_idx], dtype=torch.int64),\n",
    "        torch.tensor([pad_idx] * enc_num_pad, dtype=torch.int64)\n",
    "    ], dim=0\n",
    ").to(device)\n",
    "\n",
    "# Create the encoder mask\n",
    "encoder_mask = (encoder_input != pad_idx).unsqueeze(0).unsqueeze(0).int().to(device)  # (1, 1, MAX_SRC_LEN)\n",
    "\n",
    "# Assuming transformer is already defined and initialized\n",
    "transformer.eval()\n",
    "with torch.no_grad():\n",
    "    # Encode the input sequence\n",
    "    encoder_output = transformer.encode(encoder_input, encoder_mask)\n",
    "    \n",
    "    # Initialize the target sequence with the start token\n",
    "    tgt = torch.ones(1, 1).fill_(sos_idx).type_as(encoder_input)\n",
    "    \n",
    "    for i in range(1, MAX_TGT_LEN):\n",
    "        # Create the decoder mask for the current length of the target sequence\n",
    "        deco_mask = decoder_mask(tgt.size(1)).type_as(encoder_mask).to(device)\n",
    "        \n",
    "        # Decode the target sequence\n",
    "        out = transformer.decode(encoder_output, encoder_mask, tgt, deco_mask)\n",
    "        \n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits = out[:, -1, :]\n",
    "        \n",
    "        # Apply top-k sampling to select the next token\n",
    "        next_token = top_k_sampling(logits, k=5, temperature=1.0)  # Adjust k and temperature as needed\n",
    "        \n",
    "        # Append the next token to the target sequence\n",
    "        tgt = torch.cat([tgt, torch.ones(1, 1).type_as(encoder_input).fill_(next_token)], dim=1)\n",
    "        \n",
    "        # Break if the end of sequence token is generated\n",
    "        if next_token == eos_idx:\n",
    "            break\n",
    "\n",
    "# The generated sequence of token IDs\n",
    "print(\"Generated sequence of token IDs:\", tgt)\n",
    "\n",
    "# Decode the generated token IDs to text\n",
    "generated_text = tokenizer.decode(tgt[0].cpu().numpy().tolist(), skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Often times I find myself thinking scary thoughts and sometimes I even scare myself into thinking that something bad is going to happen to me. Once it starts, the thought continues going through my head and I can't get it out. How can I stop these thoughts?\"\n",
    "\n",
    "token_ids = tokenizer.encode(input_text).ids\n",
    "\n",
    "enc_num_pad = MAX_SRC_LEN - len(token_ids) - 2\n",
    "\n",
    "sos_idx = tokenizer.token_to_id('[SOS]')\n",
    "eos_idx = tokenizer.token_to_id('[EOS]')\n",
    "pad_idx = tokenizer.token_to_id('[PAD]')\n",
    "\n",
    "encoder_input = torch.cat(\n",
    "        [\n",
    "            torch.tensor([sos_idx], dtype = torch.int64),\n",
    "            torch.tensor(token_ids, dtype=torch.int64),\n",
    "            torch.tensor([eos_idx], dtype = torch.int64),\n",
    "            torch.tensor([pad_idx] * enc_num_pad, dtype = torch.int64)\n",
    "        ], dim = 0\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "encoder_mask = (encoder_input != pad_idx).unsqueeze(0).unsqueeze(0).int().to(device) # (1, 1, 200)\n",
    "\n",
    "\n",
    "transformer.eval()\n",
    "with torch.no_grad():\n",
    "    # 1) encoder_output\n",
    "    encoder_output = transformer.encode(encoder_input, encoder_mask)\n",
    "    tgt = torch.ones(1, 1).fill_(sos_idx).type_as(encoder_input)\n",
    "    for i in range(1, MAX_TGT_LEN):\n",
    "        deco_mask = decoder_mask(tgt.size(1)).type_as(encoder_mask).to(device)\n",
    "        out = transformer.decode(encoder_output, encoder_mask, tgt, deco_mask)\n",
    "        out_token = torch.argmax(out)\n",
    "        \n",
    "        tgt = torch.cat([tgt, torch.ones(1, 1).type_as(encoder_mask).fill_(out_token)], dim=1)\n",
    "#         print(torch.ones(1, 1).type_as(encoder_mask).fill_(out_token))\n",
    "#         print('out', out)\n",
    "#         print('deco mask', deco_mask)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tgt.tolist(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
